{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† MetaForge: Compare Solvers Colab Demo\n",
    "\n",
    "This notebook demonstrates how to compare multiple solvers (e.g., Tabu Search, GA, SA, ACO, etc.) across benchmark problems using MetaForge.\n",
    "\n",
    "---\n",
    "## ‚úÖ Install MetaForge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install metaforge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711bfc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from metaforge.utils.compare_solvers import compare_all_benchmarks\n",
    "from metaforge.utils.plotting import plot_convergence_comparison, plot_solver_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÅ Temporary override of compare_all_benchmarks with URL support\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from metaforge.problems.benchmark_loader import load_job_shop_instance\n",
    "from metaforge.metaforge_runner import run_solver\n",
    "from metaforge.utils.timer import Timer\n",
    "\n",
    "# === Pretty name mapping for solvers ===\n",
    "pretty_names = {\n",
    "    \"sa\": \"Simulated Annealing\",\n",
    "    \"ts\": \"Tabu Search\",\n",
    "    \"ga\": \"Genetic Algorithm\",\n",
    "    \"aco\": \"Ant Colony Optimization\",\n",
    "    \"q\": \"Q-Learning\",\n",
    "    \"dqn-naive\": \"DQN (naive)\",\n",
    "    \"dqn-replay\": \"DQN (replay)\",\n",
    "    \"neuroevo\": \"Neuroevolution\",\n",
    "}\n",
    "\n",
    "def compare_all_benchmarks(\n",
    "    benchmark_source,\n",
    "    solvers,\n",
    "    format=\"orlib\",\n",
    "    output_csv=\"results/benchmark_comparison.csv\",\n",
    "    track_schedule=False,\n",
    "    plot=False\n",
    "):\n",
    "    is_url = benchmark_source.startswith(\"http://\") or benchmark_source.startswith(\"https://\")\n",
    "\n",
    "    if output_csv:\n",
    "        output_dir = os.path.dirname(output_csv)\n",
    "        if output_dir:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    benchmark_files = [\n",
    "        \"ft06.txt\", \"ft10.txt\", \"ft20.txt\",\n",
    "        \"la01.txt\", \"la02.txt\", \"la03.txt\",\n",
    "        \"la04.txt\", \"la05.txt\"\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for benchmark_file in benchmark_files:\n",
    "        path = (\n",
    "            f\"{benchmark_source.rstrip('/')}/{benchmark_file}\" if is_url\n",
    "            else os.path.join(benchmark_source, benchmark_file)\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            problem = load_job_shop_instance(path, format=format)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to load {benchmark_file}: {e}\")\n",
    "            continue\n",
    "\n",
    "        for solver in solvers:\n",
    "            solver_label = pretty_names.get(solver, solver)\n",
    "            print(f\"Running {solver_label} on {benchmark_file}...\")\n",
    "\n",
    "            timer = Timer()\n",
    "            result = run_solver(\n",
    "                solver,\n",
    "                problem,\n",
    "                track_schedule=track_schedule\n",
    "            )\n",
    "            elapsed = timer.stop()\n",
    "\n",
    "            results.append({\n",
    "                \"benchmark\": benchmark_file,\n",
    "                \"solver\": solver_label,\n",
    "                \"best_score\": result[\"makespan\"],\n",
    "                \"runtime_sec\": elapsed,\n",
    "                \"best_solution\": result[\"solution\"],\n",
    "                \"all_schedules\": result[\"schedules\"],\n",
    "                \"history\": result[\"history\"],\n",
    "            })\n",
    "\n",
    "    # Write summary CSV\n",
    "    if output_csv:\n",
    "        with open(output_csv, \"w\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"benchmark\", \"solver\", \"best_score\", \"runtime_sec\"])\n",
    "            writer.writeheader()\n",
    "            for row in results:\n",
    "                writer.writerow({\n",
    "                    \"benchmark\": row[\"benchmark\"],\n",
    "                    \"solver\": row[\"solver\"],\n",
    "                    \"best_score\": row[\"best_score\"],\n",
    "                    \"runtime_sec\": row[\"runtime_sec\"],\n",
    "                })\n",
    "        print(f\"\\n‚úÖ All results saved to {output_csv}\")\n",
    "\n",
    "    # Optional plotting\n",
    "    if plot:\n",
    "        from metaforge.utils.visualization import (\n",
    "            plot_results_from_csv,\n",
    "            plot_runtime_from_csv,\n",
    "        )\n",
    "        plot_results_from_csv(output_csv)\n",
    "        plot_runtime_from_csv(output_csv)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Run All Solvers on All Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = compare_all_benchmarks(\n",
    "    benchmark_source='https://raw.githubusercontent.com/Mageed-Ghaleb/MetaForge/main/data/benchmarks/',\n",
    "    solvers=['ts', 'ga', 'sa', 'aco'],\n",
    "    track_schedule=False,\n",
    "    plot=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Plot Makespan Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one benchmark (e.g., ft06.txt)\n",
    "plot_solver_convergence(results['ft06.txt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Compare Solvers Across All Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_solver_benchmark_summary(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
